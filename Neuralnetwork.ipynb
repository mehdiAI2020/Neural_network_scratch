{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from IPython.display import Image, display\n",
    "!pip install seaborn -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Image(filename='a.jpg', width=800,height=500))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH =\"mnist.pkl.gz\"\n",
    "with gzip.open(DATA_PATH, 'rb') as f:\n",
    "    (X_train, y_train), (X_valid, y_valid), (X_test,  y_test) = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# As a sanity check, we print out the size of the data.\n",
    "print('Training data shape:    ', X_train.shape)\n",
    "print('Training labels shape:  ', y_train.shape)\n",
    "print('Validation data shape:  ', X_valid.shape)\n",
    "print('Validation labels shape:', y_valid.shape)\n",
    "print('Test data shape:        ', X_test.shape)\n",
    "print('Test labels shape:      ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean vector from training data\n",
    "mu = np.mean(X_train, axis=0)\n",
    "\n",
    "# remove mean vector from all data\n",
    "X_train -= mu\n",
    "X_valid -= mu\n",
    "X_test  -= mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the mean vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mu.reshape(28, 28), interpolation='nearest', cmap=plt.cm.Greys)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Mean value of features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_\\theta(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n) = g(\\theta^T x + \\theta_0) = \\frac{1}{1 + e^{-(\\theta^T x + \\theta_0)}}$$\n",
    "\n",
    "Notice that here, we have not included $x_0 = 1$ and hence treated the bias separetly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='multi-class-lr-vectorized.png' width='75%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$scores = X \\Theta + \\theta_0$$\n",
    "where:\n",
    "- $X \\in \\mathbb{R}^{m \\times n}$\n",
    "- $\\Theta \\in \\mathbb{R}^{n \\times c}$\n",
    "- $\\theta_0 \\in \\mathbb{R}^{1 \\times c}$\n",
    "- $scores \\in \\mathbb{R}^{m \\times c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we can compute the scores using the following statement:\n",
    "```python\n",
    "    scores = X @ Theta + theta0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing scores, we have `c` predictions for each input data. By taking max over these predictions, we can easily compute the class label for each input data:\n",
    "```python\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    scores = X @ W + b\n",
    "    return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return 100. * np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 10                # number of classes \n",
    "n = X_train.shape[1]  # number of features\n",
    "\n",
    "# init parameters randomly\n",
    "W = 0.01 * np.random.randn(n, c)\n",
    "b = np.zeros(c)\n",
    "\n",
    "# predict classes and compute accuracy\n",
    "y_pred = predict(W, b, X_train)\n",
    "acc = accuracy(y_pred, y_train)  # this function is defined in utils.py\n",
    "print(\"Accuracy = {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='softmax_classifier.png' width='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$probs = \\text{softmax}(scores)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{softmax}(x^{(i)})_k = \\frac{e^{s_k}}{\\sum_{j=1}^{c} e^{s_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data:**\n",
    "- $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{m}$,\n",
    "- $x^{(i)} \\in \\mathbb{R}^{n}$,\n",
    "- $y \\in \\{1, 2, \\dots, c\\}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- weights: $W \\in \\mathbb{R}^{n \\times c}$\n",
    "- biases: $b \\in \\mathbb{R}^{c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function:** softmax loss function + regularization\n",
    "$$L(W, b) = \\frac{1}{m} \\sum_{i=1}^{m} -\\log(p_{y^{(i)}}) + \\frac{\\lambda}{2} \\lVert W \\rVert_{2}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need to find the optimal $W$ and $b$ by minimizing $L(W, b)$. Again, we can use **gradient descent** algorithm or any other optimization method to minimize this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L_i}{\\partial w^{(k)}} = \\frac{\\partial L_i}{\\partial s_k} \\frac{\\partial s_k}{\\partial w^{(k)}} = \\frac{\\partial L_i}{\\partial s_k} x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $k = y^{(i)}$:\n",
    "$$\\frac{\\partial L_i}{\\partial s_k} = p_k - 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $k \\neq y^{(i)}$:\n",
    "$$\\frac{\\partial L_i}{\\partial s_k} = p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(W, b, X_batch, y_batch, mode='train'):\n",
    "    bs = X_batch.shape[0]  # batch size\n",
    "    \n",
    "    scores = X_batch @ W + b\n",
    "    probs = softmax(scores)\n",
    "    loss = -np.sum(np.log(probs[range(bs), y_batch])) / bs\n",
    "    \n",
    "    if mode == 'test':\n",
    "        return loss\n",
    "    \n",
    "    # compute gradients w.r.t scores\n",
    "    dscores = np.copy(probs)\n",
    "    dscores[range(bs), y_batch] -= 1.0\n",
    "    dscores /= bs\n",
    "    \n",
    "    # compute gradients w.r.t W and b\n",
    "    db = dscores.sum(axis=0)\n",
    "    dW = X_batch.T @ dscores\n",
    "    \n",
    "    return loss, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='c.png', width=500)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train, y_train, X_valid, y_valid, batch_size=32, \n",
    "                                alpha=0.01, lmbda=1e-4, num_epochs=100):\n",
    "    \n",
    "    m, n = X_train.shape\n",
    "    num_batches = m % batch_size\n",
    "    \n",
    "    report = \"Epoch {:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "    \n",
    "    # init parameters randomly\n",
    "    W = np.random.randn(n, 10) * 0.001\n",
    "    b = np.zeros((10,))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            # select a random mini-batch\n",
    "            idx = np.random.choice(m, batch_size, replace=False)\n",
    "            X_batch, y_batch = X_train[idx], y_train[idx]\n",
    "            \n",
    "            # compute loss and gradient\n",
    "            loss, dW, db = softmax_loss(W, b, X_batch, y_batch)  # data loss\n",
    "            loss += 0.5 * lmbda * np.sum(W ** 2)                 # regularization loss\n",
    "            dW += lmbda * W\n",
    "            \n",
    "            train_loss += loss\n",
    "            \n",
    "            # update parameters            \n",
    "            b = b - alpha * db\n",
    "            W = W - alpha * dW\n",
    "        \n",
    "        # report stats after each epoch\n",
    "        train_loss /= num_batches        \n",
    "        valid_loss = softmax_loss(W, b, X_valid, y_valid, mode='test')\n",
    "        print(report.format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "alpha = 1e-2\n",
    "lmbda = 1e-4\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# run mini-batch gradient descent\n",
    "W, b = mini_batch_gradient_descent(X_train, y_train, X_valid, y_valid, \n",
    "                                   batch_size=batch_size, alpha=alpha,\n",
    "                                   lmbda=lmbda, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy(predict(W, b, X_train), y_train)\n",
    "valid_acc = accuracy(predict(W, b, X_valid), y_valid)\n",
    "\n",
    "print('Training accuracy =   {:.2f}%'.format(train_acc))\n",
    "print('Validation accuracy = {:.2f}%'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy(predict(W, b, X_test), y_test)\n",
    "print('Test accuracy =   {:.2f}%'.format(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(W[:, i].reshape((28, 28)), cmap=plt.cm.coolwarm)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"%d\" % i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Non-linear Classification: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Softmax_loss(scores, y, mode='train'):\n",
    "    m = scores.shape[0]\n",
    "    probs = softmax(scores)\n",
    "    loss = -np.sum(np.log(probs[range(m), y])) / m\n",
    "    \n",
    "    if mode != 'train':\n",
    "        return loss\n",
    "    \n",
    "    # backward\n",
    "    dscores = probs\n",
    "    dscores[range(m), y] -= 1.0\n",
    "    dscores /= m\n",
    "    \n",
    "    return loss, dscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='b.png', width=500)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoLayerNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_features=784, num_hiddens=20, num_classes=10):\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # random initialization: create random weights, set all biases to zero\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(num_features, num_hiddens) * 0.001\n",
    "        self.params['W2'] = np.random.randn(num_hiddens,  num_classes) * 0.001\n",
    "        self.params['b1'] = np.zeros((num_hiddens,))\n",
    "        self.params['b2'] = np.zeros((num_classes,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # forward step\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # forward step\n",
    "        h_in = X @ W1 + b1       # hidden layer input\n",
    "        h = np.maximum(0, h_in)  # hidden layer output (using ReLU)\n",
    "        scores = h @ W2 + b2     # neural net output\n",
    "        \n",
    "        return scores\n",
    "                            \n",
    "    def train_step(self, X, y):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # forward step\n",
    "        h_in = X @ W1 + b1       # hidden layer input\n",
    "        h = np.maximum(0, h_in)  # hidden layer output (using ReLU)\n",
    "        scores = h @ W2 + b2     # neural net output\n",
    "        \n",
    "        # compute loss\n",
    "        loss, dscores = Softmax_loss(scores, y)\n",
    "        \n",
    "        # backward step\n",
    "        db2 = dscores.sum(axis=0)\n",
    "        dW2 = h.T @ dscores\n",
    "        \n",
    "        dh = dscores @ W2.T\n",
    "        dh[h_in < 0] = 0.0\n",
    "        db1 = dh.sum(axis=0)\n",
    "        dW1 = X.T @ dh\n",
    "        \n",
    "        gradient = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "                \n",
    "        return loss, gradient\n",
    "        \n",
    "    def train(self, X_train, y_train, X_valid, y_valid, batch_size=50, \n",
    "              alpha=0.001, lmbda=0.0001, num_epochs=10):\n",
    "        \n",
    "        m, n = X_train.shape        \n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        report = \"{:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for _ in range(num_batches):\n",
    "                W1, b1 = self.params['W1'], self.params['b1']\n",
    "                W2, b2 = self.params['W2'], self.params['b2']\n",
    "                \n",
    "                # select a random mini-batch\n",
    "                batch_idx = np.random.choice(m, batch_size, replace=False)\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "\n",
    "                # train on mini-batch\n",
    "                data_loss, gradient = self.train_step(X_batch, y_batch)\n",
    "                reg_loss = 0.5 * (np.sum(W1 ** 2) + np.sum(W2 ** 2))\n",
    "                train_loss += (data_loss + lmbda * reg_loss)\n",
    "                losses.append(data_loss + lmbda * reg_loss)\n",
    "\n",
    "                # regularization\n",
    "                gradient['W1'] += lmbda * W1\n",
    "                gradient['W2'] += lmbda * W2\n",
    "\n",
    "                # update parameters\n",
    "                for p in self.params:\n",
    "                    self.params[p] = self.params[p] - alpha * gradient[p]\n",
    "            \n",
    "            # report training loss and validation loss\n",
    "            train_loss /= num_batches\n",
    "            valid_loss = Softmax_loss(self.forward(X_valid), y_valid, mode='test')\n",
    "            print(report.format(epoch + 1, train_loss, valid_loss))\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict probabilties of classes for each input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = TwoLayerNeuralNetwork(num_hiddens=20)\n",
    "losses = mlp.train(X_train, y_train, X_valid, y_valid, \n",
    "                   alpha=0.05, lmbda=0.001, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define 5 configurations\n",
    "import matplotlib.pyplot as plt\n",
    "configurations = [\n",
    "    {'num_hiddens': 10, 'alpha': 0.01, 'lambda': 0.0001},\n",
    "    {'num_hiddens': 20, 'alpha': 0.05, 'lambda': 0.001},\n",
    "    {'num_hiddens': 30, 'alpha': 0.1, 'lambda': 0.01},\n",
    "    {'num_hiddens': 40, 'alpha': 0.2, 'lambda': 0.1},\n",
    "    {'num_hiddens': 50, 'alpha': 0.3, 'lambda': 1}\n",
    "]\n",
    "training_accuracies = []\n",
    "test_accuracies=[]\n",
    "lossess=[]\n",
    "\n",
    "# Initialize a dictionary to store the losses for each configuration\n",
    "config_losses = {}\n",
    "\n",
    "# Loop through the defined configurations\n",
    "for config in configurations:\n",
    "    # Extract the parameters for the current configuration\n",
    "    num_hiddens = config['num_hiddens']\n",
    "    alpha = config['alpha']\n",
    "    lambda_ = config['lambda']  # Using lambda_ to avoid conflict with the keyword lambda\n",
    "\n",
    "    # Initialize the neural network with the current configuration\n",
    "    mlp = TwoLayerNeuralNetwork(num_hiddens=num_hiddens)\n",
    "    \n",
    "    # Train the neural network and get the losses\n",
    "    losses = mlp.train(X_train, y_train, X_valid, y_valid, alpha=alpha, lmbda=lambda_, num_epochs=10)\n",
    "    lossess.append(losses)\n",
    "    # Store the losses with the corresponding configuration\n",
    "    config_losses[(num_hiddens, alpha, lambda_)] = losses\n",
    "    #predictions = mlp.predict(X_train)\n",
    "    train_acc = accuracy(mlp.predict(X_train), y_train)\n",
    "    training_accuracies.append(train_acc)\n",
    "\n",
    "    test_acc = accuracy(mlp.predict(X_test), y_test)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "\n",
    "    # Print the configuration and its final loss for comparison\n",
    "    print(f\"Config: num_hiddens={num_hiddens}, alpha={alpha}, lambda={lambda_}, Final Loss: {losses[-1]}\")\n",
    "\n",
    "labels = [f\"Config {i+1}\" for i in range(len(configurations))]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, training_accuracies, color='skyblue')\n",
    "\n",
    "# Add title and labels with more information\n",
    "plt.title('Training Accuracies of Different Configurations')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Training Accuracy')\n",
    "\n",
    "# Optional: Display the accuracy values on top of the bars\n",
    "for i, acc in enumerate(training_accuracies):\n",
    "    plt.text(i, acc, f\"{acc:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, test_accuracies, color='skyblue')\n",
    "\n",
    "# Add title and labels with more information\n",
    "plt.title('Test Accuracies of Different Configurations')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "# Optional: Display the accuracy values on top of the bars\n",
    "for i, acc in enumerate(test_accuracies):\n",
    "    plt.text(i, acc, f\"{acc:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I plot loss function for each configuration seperatly\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    plt.plot(lossess[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(0, 10001, 1000), range(0, 11))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(20,), learning_rate='adaptive', alpha=1.0, max_iter=10, verbose=1)\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.score(X_train, y_train)\n",
    "print(\"Train accuracy = {:.2f}%\".format(train_acc * 100))\n",
    "\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(\"Test accuracy  = {:.2f}%\".format(test_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming 'y_true' and 'y_pred' are defined\n",
    "# Compute the confusion matrix\n",
    "y_pre=model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pre)\n",
    "\n",
    "unique_labels = np.unique(y_test)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fd24dc732891b5f87b0cf4d9c4a9ecb93a25a51929f0934071c5bb9199a7603"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
